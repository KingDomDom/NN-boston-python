{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用VS Code+jupyter notebook编译\n",
    "使用Nvidia RTX3060 GPU进行训练\n",
    "作者:张伟业\n",
    "'''\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "基于BP算法，使用Numpy从零编程实现多层感知机（全连接）网络，利用波士顿房价数据集进行训练和测试。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从零构建全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 网络模型基类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items(): setattr(self, key, value)\n",
    "        # 默认为可训练状态\n",
    "        self.training = True\n",
    "        # 保存一些必要的数据用于反向传播\n",
    "        self.storage = {}\n",
    "    '''前向传播调用'''\n",
    "    def __call__(self, x, **kwargs):\n",
    "        self.storage.update({'x': x})\n",
    "        x = self.forward(x, **kwargs)\n",
    "        return x\n",
    "    '''定义前向传播'''\n",
    "    def forward(self):\n",
    "        raise NotImplementedError('not to be implemented')\n",
    "    '''定义反向传播'''\n",
    "    def backward(self):\n",
    "        raise NotImplementedError('not to be implemented')\n",
    "    '''返回模型的名字'''\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "    '''设置为训练状态'''\n",
    "    def train(self, mode=True):\n",
    "        def apply(module_dict, mode):\n",
    "            for module in module_dict.values():\n",
    "                if isinstance(module, dict):\n",
    "                    apply(module, mode)\n",
    "                else:\n",
    "                    setattr(module, 'training', mode)\n",
    "        module_dict = self.modules()\n",
    "        apply(module_dict, mode)\n",
    "    '''设置为测试状态'''\n",
    "    def eval(self):\n",
    "        self.train(mode=False)\n",
    "    '''根据使用的优化器更新参数'''\n",
    "    def update(self):\n",
    "        raise NotImplementedError('not to be implemented')\n",
    "    '''返回所有modules'''\n",
    "    def modules(self):\n",
    "        module_dict, attrs = {self.name(): self}, dir(self)\n",
    "        for attr in attrs:\n",
    "            value = getattr(self, attr)\n",
    "            if isinstance(value, Module):\n",
    "                module_dict[f'{self.name()}.{attr}'] = value.modules()\n",
    "        return module_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 线性层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__(\n",
    "            in_features=in_features, out_features=out_features, bias=bias\n",
    "        )\n",
    "        # 初始化权重\n",
    "        thresh = 1 / math.sqrt(in_features)\n",
    "        self.weight = np.random.uniform(\n",
    "            -thresh, thresh, (in_features, out_features)\n",
    "        )\n",
    "        self.bias = np.zeros((1, out_features))\n",
    "        # 初始化storage\n",
    "        self.storage.update({\n",
    "            'direction': {\n",
    "                'weight': np.zeros(np.shape(self.weight)),\n",
    "                'bias': np.zeros(np.shape(self.bias))\n",
    "            },\n",
    "        })\n",
    "        # 取消bias\n",
    "        if not bias: delattr(self, 'bias')\n",
    "    '''定义前向传播'''\n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'bias'):\n",
    "            # feats = x · weight + bias\n",
    "            \n",
    "            feats = np.dot(x, self.weight) + self.bias\n",
    "        else:\n",
    "            # feats = x · weight\n",
    "            \n",
    "            feats = np.dot(x, self.weight)\n",
    "        print('forward is well')\n",
    "        return feats\n",
    "    '''定义反向传播'''\n",
    "    def backward(self, accumulated_gradient):\n",
    "        weight = self.weight\n",
    "        if self.training:\n",
    "            # 计算梯度\n",
    "            # grad_w = x^T · accumulated_gradient\n",
    "            # hint: get x from self.storage['x']\n",
    "            \n",
    "            x = self.storage['x']\n",
    "            grad_w = np.dot(x.T, accumulated_gradient)\n",
    "            grad_b = np.sum(accumulated_gradient, axis=0, keepdims=True)\n",
    "            # 根据梯度更新weight\n",
    "            results = self.update(self.weight, grad_w, self.storage['direction']['weight'])\n",
    "            self.weight, self.storage['direction']['weight'] = results['params'], results['direction']\n",
    "            # 根据梯度更新bias\n",
    "            if hasattr(self, 'bias'):\n",
    "                results = self.update(self.bias, grad_b, self.storage['direction']['bias'])\n",
    "                self.bias, self.storage['direction']['bias'] = results['params'], results['direction']\n",
    "        return accumulated_gradient.dot(weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "    '''定义前向传播'''\n",
    "    def forward(self, x):\n",
    "        # calculate out = sigmoid(x)\n",
    "        \n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        return out\n",
    "    '''定义反向传播'''\n",
    "    def backward(self, accumulated_gradient):\n",
    "        z = self.forward(self.storage['x'])\n",
    "        # calculate gradient with z\n",
    "        \n",
    "        gradient = z * (1 - z)\n",
    "        return accumulated_gradient * gradient\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "    '''定义前向传播'''\n",
    "    def forward(self, x):\n",
    "        # calculate out = ReLU(x)\n",
    "        \n",
    "        out = np.maximum(0, x)\n",
    "        return out\n",
    "    '''定义反向传播'''\n",
    "    def backward(self, accumulated_gradient):\n",
    "        gradient = np.where(self.storage['x'] >= 0, 1, 0)\n",
    "        return accumulated_gradient * gradient\n",
    "\n",
    "class Softmax(Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(Softmax, self).__init__(dim=dim)\n",
    "    '''定义前向传播'''\n",
    "    def forward(self, x):\n",
    "        ex = np.exp(x - np.max(x, axis=self.dim, keepdims=True))\n",
    "        return ex / np.sum(ex, axis=-1, keepdims=True)\n",
    "    '''定义反向传播'''\n",
    "    def backward(self, accumulated_gradient):\n",
    "        p = self.forward(self.storage['x'])\n",
    "        gradient = p * (1 - p)\n",
    "        return accumulated_gradient * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''均方误差损失'''\n",
    "class MSELoss():\n",
    "    def __init__(self, reduction='mean'):\n",
    "        assert reduction in ['sum', 'none', 'mean']\n",
    "        self.reduction = reduction\n",
    "        self.storage = {}\n",
    "    '''定义前向传播'''\n",
    "    def __call__(self, predictions, targets):\n",
    "        self.storage.update({\n",
    "            'predictions': predictions, 'targets': targets\n",
    "        })\n",
    "        # calculate MSE loss\n",
    "        \n",
    "        loss = np.mean((predictions - targets) ** 2)\n",
    "        if self.reduction == 'none': return loss\n",
    "        loss = getattr(loss, self.reduction)()\n",
    "        return loss\n",
    "    '''定义反向传播'''\n",
    "    def backward(self):\n",
    "        # calculate gradient of MSE loss\n",
    "        # hint: get predictions and targets from self.storage['predictions'] and self.storage['targets']\n",
    "        \n",
    "        predictions, targets = self.storage['predictions'], self.storage['targets']\n",
    "        gradient = 2 * (predictions - targets)\n",
    "        if self.reduction == 'mean': gradient /= gradient.shape[0]\n",
    "        return gradient\n",
    "\n",
    "'''交叉熵损失'''\n",
    "class CrossEntropy():\n",
    "    def __init__(self, reduction='mean', eps=1e-12):\n",
    "        assert reduction in ['sum', 'none', 'mean']\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "        self.storage = {}\n",
    "    '''定义前向传播'''\n",
    "    def __call__(self, predictions, targets):\n",
    "        self.storage.update({\n",
    "            'predictions': predictions, 'targets': targets\n",
    "        })\n",
    "        predictions = np.clip(predictions, self.eps, 1 - self.eps)\n",
    "        # calculate cross entropy loss\n",
    "        \n",
    "        loss = -np.mean(targets * np.log(predictions))\n",
    "        if self.reduction == 'none': return loss\n",
    "        loss = getattr(loss, self.reduction)()\n",
    "        return loss\n",
    "    '''定义反向传播'''\n",
    "    def backward(self):\n",
    "        predictions, targets = self.storage['predictions'], self.storage['targets']\n",
    "        predictions = np.clip(predictions, self.eps, 1 - self.eps)\n",
    "        gradient = - (targets / predictions) + (1 - targets) / (1 - predictions)\n",
    "        if self.reduction == 'mean': gradient /= gradient.shape[0]\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 优化器基类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOptimizer():\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items(): setattr(self, key, value)\n",
    "    '''所有网络层都使用优化器的update函数'''\n",
    "    def applyupdate(self, module_dict):\n",
    "        for module in module_dict.values():\n",
    "            if isinstance(module, dict):\n",
    "                self.applyupdate(module)\n",
    "            else:\n",
    "                setattr(module, 'update', self.update)\n",
    "    '''梯度更新函数'''\n",
    "    def update(self, params, grads, direction):\n",
    "        raise NotImplementedError('not to be implemented')\n",
    "    '''参数更新'''\n",
    "    def step(self):\n",
    "        self.structure.backward(self.criterion.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. SGD优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(BaseOptimizer):\n",
    "    def __init__(self, structure, criterion, learning_rate=0.01, momentum=0):\n",
    "        super(SGD, self).__init__(\n",
    "            structure=structure, criterion=criterion, learning_rate=learning_rate, momentum=momentum\n",
    "        )\n",
    "        # 所有网络层都使用优化器的update函数\n",
    "        self.applyupdate(self.structure.modules())\n",
    "    '''更新函数'''\n",
    "    def update(self, params, grads, direction):\n",
    "        direction = self.momentum * direction + (1 - self.momentum) * grads\n",
    "        params = params - self.learning_rate * direction\n",
    "        return {\n",
    "            'params': params, 'direction': direction\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Sequential模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''定义Sequential模型'''\n",
    "class Sequential(Module):\n",
    "    def __init__(self, module_list=[], **kwargs):\n",
    "        super(Sequential, self).__init__(**kwargs)\n",
    "        self.module_dict = {}\n",
    "        for idx, module in enumerate(module_list):\n",
    "            self.module_dict.update({str(idx): module})\n",
    "    '''添加模型'''\n",
    "    def addmodule(self, name, module):\n",
    "        self.module_dict.update({name: module})\n",
    "    '''前向传播'''\n",
    "    def forward(self, x):\n",
    "        for module in self.module_dict.values():\n",
    "            x = module(x)\n",
    "        return x\n",
    "    '''反向传播'''\n",
    "    def backward(self, accumulated_gradient):\n",
    "        for module in reversed(list(self.module_dict.values())):\n",
    "            accumulated_gradient = module.backward(accumulated_gradient)\n",
    "        return accumulated_gradient\n",
    "    '''返回所有modules'''\n",
    "    def modules(self):\n",
    "        return self.module_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "波士顿房价预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\myenvs\\moon\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "train = boston.data\n",
    "target = boston.target\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
       "5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
       "6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
       "7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
       "8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
       "9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     15.3  396.90   4.98    24.0  \n",
       "1     17.8  396.90   9.14    21.6  \n",
       "2     17.8  392.83   4.03    34.7  \n",
       "3     18.7  394.63   2.94    33.4  \n",
       "4     18.7  396.90   5.33    36.2  \n",
       "5     18.7  394.12   5.21    28.7  \n",
       "6     15.2  395.60  12.43    22.9  \n",
       "7     15.2  396.90  19.15    27.1  \n",
       "8     15.2  386.63  29.93    16.5  \n",
       "9     15.2  386.71  17.10    18.9  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "data_df['target'] = boston.target\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "\n",
    "model = Sequential()\n",
    "# define the first layer as a linear layer, in_features = X_train.shape[1], out_features = hidden_dim\n",
    "\n",
    "linear_layer1 = Linear(in_features = X_train.shape[1], out_features = hidden_dim)\n",
    "model.addmodule('fc1', linear_layer1)\n",
    "# define the activation function as ReLU\n",
    "\n",
    "relu_layer = ReLU()\n",
    "model.addmodule('relu', relu_layer)\n",
    "# define the second layer as a linear layer, in_features = hidden_dim, out_features = 1 (the price)\n",
    "\n",
    "linear_layer2 = Linear(in_features = hidden_dim, out_features = 1)\n",
    "model.addmodule('fc2', linear_layer2)\n",
    "# as this is a regression problem, we don't need a softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "criterion = MSELoss()\n",
    "# define the optimizer\n",
    "optimizer = SGD(model, criterion, 0.001, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, X_train, y_train, batch_size):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i in range(math.ceil(X_train[0].shape[0] / batch_size)):\n",
    "        X_batch = X_train[i * batch_size: (i + 1) * batch_size]\n",
    "        y_batch = y_train[i * batch_size: (i + 1) * batch_size]\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, X_test, y_test, batch_size):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i in range(math.ceil(X_test.shape[0] / batch_size)):\n",
    "        X_batch = X_test[i * batch_size: (i + 1) * batch_size]\n",
    "        y_batch = y_test[i * batch_size: (i + 1) * batch_size]\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        losses.append(loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 1, Train Loss: 12162.261280297917, Test Loss: 2913070.032634695\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 2, Train Loss: 3194363.802829862, Test Loss: 614.3200129039325\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 3, Train Loss: 547.8751467670966, Test Loss: 628.0040370464917\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 4, Train Loss: 560.483611168316, Test Loss: 640.2595486019927\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 5, Train Loss: 571.7870700430448, Test Loss: 651.193241101083\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 6, Train Loss: 581.8800566264106, Test Loss: 660.9080556738899\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 7, Train Loss: 590.8545455132186, Test Loss: 669.5022662009975\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 8, Train Loss: 598.7989206262178, Test Loss: 677.0688966039045\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 9, Train Loss: 605.7972868268006, Test Loss: 683.695392152442\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 10, Train Loss: 611.9290459164599, Test Loss: 689.4634828609607\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 11, Train Loss: 617.2686740824939, Test Loss: 694.4491900553338\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 12, Train Loss: 621.885650951645, Test Loss: 698.7229376294558\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 13, Train Loss: 625.8445009421462, Test Loss: 702.3497378678852\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 14, Train Loss: 629.2049160439126, Test Loss: 705.3894283916384\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 15, Train Loss: 632.0219359102923, Test Loss: 707.8969421118131\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 16, Train Loss: 634.3461665385855, Test Loss: 709.922596313763\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 17, Train Loss: 636.2240231142154, Test Loss: 711.5123903558876\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 18, Train Loss: 637.69798600855, Test Loss: 712.7083041239358\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 19, Train Loss: 638.806861625715, Test Loss: 713.5485914732853\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 20, Train Loss: 639.5860419290277, Test Loss: 714.0680645303719\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 21, Train Loss: 640.0677581558484, Test Loss: 714.2983660011093\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 22, Train Loss: 640.2813255418798, Test Loss: 714.2682276219252\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 23, Train Loss: 640.2533768958103, Test Loss: 714.0037136469489\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 24, Train Loss: 640.0080836520411, Test Loss: 713.5284488403562\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 25, Train Loss: 639.567363630792, Test Loss: 712.8638308740694\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 26, Train Loss: 638.9510751896148, Test Loss: 712.0292273485394\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 27, Train Loss: 638.1771977890785, Test Loss: 711.0421578827242\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 28, Train Loss: 637.261999242928, Test Loss: 709.9184618782658\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 29, Train Loss: 636.2201900991541, Test Loss: 708.6724526678975\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 30, Train Loss: 635.0650657190163, Test Loss: 707.3170588217631\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 31, Train Loss: 633.8086366987372, Test Loss: 705.8639534175068\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 32, Train Loss: 632.461748323434, Test Loss: 704.3236720885104\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 33, Train Loss: 631.0341897627723, Test Loss: 702.7057206557174\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 34, Train Loss: 629.5347937192042, Test Loss: 701.0186731269114\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 35, Train Loss: 627.9715272273461, Test Loss: 699.2702608169124\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 36, Train Loss: 626.3515742810386, Test Loss: 697.4674533058496\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 37, Train Loss: 624.6814109358638, Test Loss: 695.6165319127134\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 38, Train Loss: 622.9668735017591, Test Loss: 693.7231563195094\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 39, Train Loss: 621.2132204046239, Test Loss: 691.7924249388454\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 40, Train Loss: 619.4251882588635, Test Loss: 689.8289295755995\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 41, Train Loss: 617.6070426556248, Test Loss: 687.8368048921866\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 42, Train Loss: 615.762624134861, Test Loss: 685.8197731473217\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 43, Train Loss: 613.8953897737899, Test Loss: 683.7811846404095\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 44, Train Loss: 612.0084507902255, Test Loss: 681.7240542579916\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 45, Train Loss: 610.1046065268489, Test Loss: 679.6510944851431\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 46, Train Loss: 608.1863751519421, Test Loss: 677.5647452134086\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 47, Train Loss: 606.2560213834802, Test Loss: 675.4672006477488\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 48, Train Loss: 604.3155815167901, Test Loss: 673.360433588029\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 49, Train Loss: 602.3668860112277, Test Loss: 671.2462173357114\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 50, Train Loss: 600.4115798684254, Test Loss: 669.126145453535\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 51, Train Loss: 598.4511410135644, Test Loss: 667.0016495849723\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 52, Train Loss: 596.4868968717324, Test Loss: 664.8740155210292\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 53, Train Loss: 594.5200393136477, Test Loss: 662.7443976843715\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 54, Train Loss: 592.5516381287623, Test Loss: 660.6138321847425\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 55, Train Loss: 590.582653168898, Test Loss: 658.4832485850138\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 56, Train Loss: 588.6139452920191, Test Loss: 656.3534805039271\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 57, Train Loss: 586.6462862234143, Test Loss: 654.2252751694978\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 58, Train Loss: 584.6803674403328, Test Loss: 652.0993020260835\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 59, Train Loss: 582.716808175936, Test Loss: 649.9761604881705\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 60, Train Loss: 580.7561626291672, Test Loss: 647.8563869249057\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 61, Train Loss: 578.7989264587624, Test Loss: 645.7404609512364\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 62, Train Loss: 576.8455426320184, Test Loss: 643.6288110941207\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 63, Train Loss: 574.896406692058, Test Loss: 641.5218198955865\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 64, Train Loss: 572.9518715011021, Test Loss: 639.4198285083546\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 65, Train Loss: 571.0122515116282, Test Loss: 637.323140834291\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 66, Train Loss: 569.0778266122044, Test Loss: 635.2320272509935\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 67, Train Loss: 567.1488455901855, Test Loss: 633.1467279673752\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 68, Train Loss: 565.2255292493085, Test Loss: 631.067456045064\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 69, Train Loss: 563.3080732164664, Test Loss: 628.9944001188117\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 70, Train Loss: 561.3966504685604, Test Loss: 626.9277268458228\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 71, Train Loss: 559.4914136072716, Test Loss: 624.8675831109508\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 72, Train Loss: 557.592496906836, Test Loss: 622.81409801205\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 73, Train Loss: 555.7000181574267, Test Loss: 620.7673846473501\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 74, Train Loss: 553.8140803244987, Test Loss: 618.7275417245696\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 75, Train Loss: 551.9347730424361, Test Loss: 616.6946550095086\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 76, Train Loss: 550.0621739590156, Test Loss: 614.6687986301148\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 77, Train Loss: 548.196349945567, Test Loss: 612.6500362504189\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 78, Train Loss: 546.337358186217, Test Loss: 610.638422127308\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 79, Train Loss: 544.4852471582885, Test Loss: 608.634002061813\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 80, Train Loss: 542.6400575147101, Test Loss: 606.6368142554284\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 81, Train Loss: 540.801822878218, Test Loss: 604.6468900809309\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 82, Train Loss: 538.9705705561578, Test Loss: 602.664254776226\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 83, Train Loss: 537.1463221838094, Test Loss: 600.6889280688963\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 84, Train Loss: 535.3290943033778, Test Loss: 598.7209247383688\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 85, Train Loss: 533.5188988850698, Test Loss: 596.7602551219185\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 86, Train Loss: 531.7157437960462, Test Loss: 594.8069255701178\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 87, Train Loss: 529.9196332224531, Test Loss: 592.8609388567718\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 88, Train Loss: 528.1305680492218, Test Loss: 590.9222945478841\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 89, Train Loss: 526.3485462018573, Test Loss: 588.9909893337408\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 90, Train Loss: 524.5735629540122, Test Loss: 587.0670173277922\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 91, Train Loss: 522.8056112042668, Test Loss: 585.1503703356511\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 92, Train Loss: 521.0446817251914, Test Loss: 583.2410380971837\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 93, Train Loss: 519.2907633874636, Test Loss: 581.3390085043858\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 94, Train Loss: 517.5438433615315, Test Loss: 579.4442677974566\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 95, Train Loss: 515.8039072990686, Test Loss: 577.5568007412508\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 96, Train Loss: 514.0709394962405, Test Loss: 575.6765907840639\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 97, Train Loss: 512.344923040601, Test Loss: 573.8036202005189\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 98, Train Loss: 510.6258399432557, Test Loss: 571.9378702201384\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 99, Train Loss: 508.91367125776475, Test Loss: 570.079321143033\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 100, Train Loss: 507.2083971871107, Test Loss: 568.2279524439922\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 101, Train Loss: 505.5099971799258, Test Loss: 566.3837428661374\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 102, Train Loss: 503.81845001705227, Test Loss: 564.5466705051749\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 103, Train Loss: 502.13373388940147, Test Loss: 562.7167128851947\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 104, Train Loss: 500.45582646798306, Test Loss: 560.8938470268516\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 105, Train Loss: 498.7847049668874, Test Loss: 559.0780495086954\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 106, Train Loss: 497.12034619992437, Test Loss: 557.2692965223321\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 107, Train Loss: 495.4627266315549, Test Loss: 555.467563922032\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 108, Train Loss: 493.8118224226859, Test Loss: 553.6728272693392\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 109, Train Loss: 492.16760947183934, Test Loss: 551.8850618731838\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 110, Train Loss: 490.5300634521634, Test Loss: 550.1042428259436\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 111, Train Loss: 488.8991598446974, Test Loss: 548.3303450358615\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 112, Train Loss: 487.27487396826655, Test Loss: 546.5633432561813\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 113, Train Loss: 485.65718100634456, Test Loss: 544.8032121113323\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 114, Train Loss: 484.0460560311861, Test Loss: 543.0499261204541\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 115, Train Loss: 482.44147402550374, Test Loss: 541.3034597185294\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 116, Train Loss: 480.843409901933, Test Loss: 539.5637872753663\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 117, Train Loss: 479.25183852050975, Test Loss: 537.8308831126374\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 118, Train Loss: 477.6667347043562, Test Loss: 536.1047215191791\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 119, Train Loss: 476.08807325375585, Test Loss: 534.3852767647194\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 120, Train Loss: 474.51582895877965, Test Loss: 532.6725231121911\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 121, Train Loss: 472.9499766106062, Test Loss: 530.9664348287761\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 122, Train Loss: 471.39049101166916, Test Loss: 529.2669861958024\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 123, Train Loss: 469.83734698474746, Test Loss: 527.5741515176126\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 124, Train Loss: 468.29051938110507, Test Loss: 525.887905129505\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 125, Train Loss: 466.74998308777583, Test Loss: 524.2082214048394\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 126, Train Loss: 465.21571303407734, Test Loss: 522.5350747613934\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 127, Train Loss: 463.68768419743435, Test Loss: 520.8684396670419\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 128, Train Loss: 462.1658716085767, Test Loss: 519.2082906448294\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 129, Train Loss: 460.65025035617873, Test Loss: 517.5546022774938\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 130, Train Loss: 459.1407955909924, Test Loss: 515.9073492114996\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 131, Train Loss: 457.63748252952814, Test Loss: 514.2665061606261\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 132, Train Loss: 456.1402864573265, Test Loss: 512.6320479091587\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 133, Train Loss: 454.6491827318626, Test Loss: 511.00394931472\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 134, Train Loss: 453.1641467851209, Test Loss: 509.3821853107795\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 135, Train Loss: 451.68515412587203, Test Loss: 507.76673090887175\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 136, Train Loss: 450.2121803416821, Test Loss: 506.1575612005546\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 137, Train Loss: 448.74520110068295, Test Loss: 504.55465135913147\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 138, Train Loss: 447.28419215312425, Test Loss: 502.957976641163\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 139, Train Loss: 445.8291293327333, Test Loss: 501.36751238778777\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 140, Train Loss: 444.379988557898, Test Loss: 499.7832340258735\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 141, Train Loss: 442.936745832694, Test Loss: 498.2051170690134\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 142, Train Loss: 441.4993772477701, Test Loss: 496.6331371183843\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 143, Train Loss: 440.06785898110627, Test Loss: 495.06726986348167\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 144, Train Loss: 438.64216729865814, Test Loss: 493.5074910827417\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 145, Train Loss: 437.2222785548986, Test Loss: 491.95377664406385\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 146, Train Loss: 435.80816919326713, Test Loss: 490.40610250524264\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 147, Train Loss: 434.3998157465361, Test Loss: 488.86444471431906\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 148, Train Loss: 432.99719483710396, Test Loss: 487.3287794098585\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 149, Train Loss: 431.6002831772194, Test Loss: 485.7990828211639\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 150, Train Loss: 430.2090575691482, Test Loss: 484.2753312684298\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 151, Train Loss: 428.82349490528367, Test Loss: 482.7575011628437\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 152, Train Loss: 427.44357216821146, Test Loss: 481.24556900664095\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 153, Train Loss: 426.06926643072836, Test Loss: 479.73951139311606\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 154, Train Loss: 424.7005548558247, Test Loss: 478.2393050065965\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 155, Train Loss: 423.33741469663016, Test Loss: 476.7449266223832\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 156, Train Loss: 421.97982329632987, Test Loss: 475.2563531066596\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 157, Train Loss: 420.62775808805145, Test Loss: 473.7735614163729\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 158, Train Loss: 419.28119659472736, Test Loss: 472.2965285990923\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 159, Train Loss: 417.94011642893605, Test Loss: 470.82523179284505\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 160, Train Loss: 416.60449529272137, Test Loss: 469.35964822593155\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 161, Train Loss: 415.27431097739577, Test Loss: 467.8997552167248\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 162, Train Loss: 413.94954136332615, Test Loss: 466.4455301734537\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 163, Train Loss: 412.6301644197074, Test Loss: 464.9969505939715\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 164, Train Loss: 411.3161582043216, Test Loss: 463.55399406551396\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 165, Train Loss: 410.00750086328696, Test Loss: 462.1166382644444\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 166, Train Loss: 408.7041706307971, Test Loss: 460.684860955991\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 167, Train Loss: 407.40614582885013, Test Loss: 459.25863999397336\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 168, Train Loss: 406.11340486697236, Test Loss: 457.83795332052387\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 169, Train Loss: 404.82592624193256, Test Loss: 456.4227789658004\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 170, Train Loss: 403.54368853745177, Test Loss: 455.013095047694\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 171, Train Loss: 402.26667042390795, Test Loss: 453.6088797715313\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 172, Train Loss: 400.99485065803464, Test Loss: 452.2101114297727\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 173, Train Loss: 399.7282080826175, Test Loss: 450.8167684017059\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 174, Train Loss: 398.46672162618586, Test Loss: 449.42882915313646\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 175, Train Loss: 397.2103703027017, Test Loss: 448.046272236076\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 176, Train Loss: 395.95913321124635, Test Loss: 446.6690762884268\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 177, Train Loss: 394.712989535705, Test Loss: 445.2972200336652\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 178, Train Loss: 393.4719185444494, Test Loss: 443.9306822805228\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 179, Train Loss: 392.2358995900188, Test Loss: 442.56944192266633\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 180, Train Loss: 391.00491210879926, Test Loss: 441.2134779383765\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 181, Train Loss: 389.7789356207039, Test Loss: 439.8627693902257\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 182, Train Loss: 388.55794972884996, Test Loss: 438.51729542475505\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 183, Train Loss: 387.34193411923707, Test Loss: 437.1770352721511\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 184, Train Loss: 386.1308685604247, Test Loss: 435.8419682459221\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 185, Train Loss: 384.92473290320885, Test Loss: 434.51207374257433\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 186, Train Loss: 383.7235070802998, Test Loss: 433.1873312412881\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 187, Train Loss: 382.5271711059993, Test Loss: 431.8677203035943\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 188, Train Loss: 381.33570507587797, Test Loss: 430.55322057305057\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 189, Train Loss: 380.1490891664535, Test Loss: 429.2438117749184\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 190, Train Loss: 378.96730363486836, Test Loss: 427.939473715841\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 191, Train Loss: 377.7903288185696, Test Loss: 426.64018628352017\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 192, Train Loss: 376.61814513498734, Test Loss: 425.3459294463958\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 193, Train Loss: 375.4507330812159, Test Loss: 424.0566832533246\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 194, Train Loss: 374.2880732336936, Test Loss: 422.77242783325994\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 195, Train Loss: 373.1301462478855, Test Loss: 421.493143394933\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 196, Train Loss: 371.9769328579647, Test Loss: 420.21881022653383\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 197, Train Loss: 370.8284138764968, Test Loss: 418.9494086953939\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 198, Train Loss: 369.6845701941235, Test Loss: 417.68491924767\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 199, Train Loss: 368.5453827792482, Test Loss: 416.4253224080278\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 200, Train Loss: 367.4108326777217, Test Loss: 415.1705987793274\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 201, Train Loss: 366.2809010125299, Test Loss: 413.9207290423097\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 202, Train Loss: 365.1555689834817, Test Loss: 412.67569395528375\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 203, Train Loss: 364.0348178668985, Test Loss: 411.43547435381504\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 204, Train Loss: 362.9186290153043, Test Loss: 410.200051150415\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 205, Train Loss: 361.8069838571174, Test Loss: 408.9694053342314\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 206, Train Loss: 360.6998638963426, Test Loss: 407.74351797073996\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 207, Train Loss: 359.59725071226507, Test Loss: 406.52237020143724\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 208, Train Loss: 358.4991259591448, Test Loss: 405.3059432435343\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 209, Train Loss: 357.40547136591306, Test Loss: 404.0942183896517\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 210, Train Loss: 356.31626873586845, Test Loss: 402.8871770075159\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 211, Train Loss: 355.2314999463759, Test Loss: 401.68480053965584\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 212, Train Loss: 354.1511469485655, Test Loss: 400.4870705031021\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 213, Train Loss: 353.07519176703283, Test Loss: 399.2939684890856\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 214, Train Loss: 352.00361649954107, Test Loss: 398.1054761627392\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 215, Train Loss: 350.93640331672304, Test Loss: 396.92157526279857\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 216, Train Loss: 349.87353446178565, Test Loss: 395.7422476013058\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 217, Train Loss: 348.81499225021435, Test Loss: 394.5674750633135\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 218, Train Loss: 347.76075906948006, Test Loss: 393.39723960658966\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 219, Train Loss: 346.71081737874545, Test Loss: 392.2315232613248\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 220, Train Loss: 345.66514970857486, Test Loss: 391.0703081298386\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 221, Train Loss: 344.62373866064246, Test Loss: 389.91357638628966\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 222, Train Loss: 343.5865669074442, Test Loss: 388.76131027638473\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 223, Train Loss: 342.5536171920089, Test Loss: 387.61349211709023\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 224, Train Loss: 341.52487232761234, Test Loss: 386.4701042963436\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 225, Train Loss: 340.5003151974903, Test Loss: 385.33112927276795\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 226, Train Loss: 339.47992875455475, Test Loss: 384.19654957538546\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 227, Train Loss: 338.4636960211103, Test Loss: 383.06634780333326\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 228, Train Loss: 337.45160008857175, Test Loss: 381.9405066255808\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 229, Train Loss: 336.4436241171828, Test Loss: 380.81900878064675\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 230, Train Loss: 335.43975133573605, Test Loss: 379.70183707631895\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 231, Train Loss: 334.43996504129416, Test Loss: 378.58897438937413\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 232, Train Loss: 333.44424859891205, Test Loss: 377.4804036652989\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 233, Train Loss: 332.4525854413597, Test Loss: 376.3761079180131\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 234, Train Loss: 331.46495906884746, Test Loss: 375.27607022959205\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 235, Train Loss: 330.48135304875103, Test Loss: 374.18027374999224\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 236, Train Loss: 329.50175101533796, Test Loss: 373.088701696777\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 237, Train Loss: 328.526136669496, Test Loss: 372.0013373548424\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 238, Train Loss: 327.5544937784613, Test Loss: 370.9181640761464\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 239, Train Loss: 326.58680617554876, Test Loss: 369.8391652794371\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 240, Train Loss: 325.623057759883, Test Loss: 368.7643244499832\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 241, Train Loss: 324.6632324961307, Test Loss: 367.6936251393053\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 242, Train Loss: 323.70731441423334, Test Loss: 366.62705096490805\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 243, Train Loss: 322.755287609142, Test Loss: 365.56458561001364\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 244, Train Loss: 321.8071362405523, Test Loss: 364.50621282329615\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 245, Train Loss: 320.8628445326415, Test Loss: 363.45191641861754\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 246, Train Loss: 319.9223967738054, Test Loss: 362.40168027476386\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 247, Train Loss: 318.9857773163972, Test Loss: 361.35548833518294\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 248, Train Loss: 318.05297057646726, Test Loss: 360.3133246077233\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 249, Train Loss: 317.12396103350375, Test Loss: 359.2751731643741\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 250, Train Loss: 316.1987332301742, Test Loss: 358.2410181410057\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 251, Train Loss: 315.2772717720688, Test Loss: 357.210843737112\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 252, Train Loss: 314.35956132744377, Test Loss: 356.18463421555265\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 253, Train Loss: 313.4455866269667, Test Loss: 355.162373902298\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 254, Train Loss: 312.5353324634621, Test Loss: 354.14404718617345\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 255, Train Loss: 311.6287836916588, Test Loss: 353.1296385186059\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 256, Train Loss: 310.72592522793764, Test Loss: 352.1191324133706\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 257, Train Loss: 309.82674205008055, Test Loss: 351.1125134463394\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 258, Train Loss: 308.9312191970208, Test Loss: 350.1097662552301\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 259, Train Loss: 308.039341768594, Test Loss: 349.1108755393561\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 260, Train Loss: 307.15109492529007, Test Loss: 348.11582605937815\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 261, Train Loss: 306.2664638880063, Test Loss: 347.124602637056\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 262, Train Loss: 305.3854339378016, Test Loss: 346.1371901550024\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 263, Train Loss: 304.5079904156519, Test Loss: 345.1535735564361\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 264, Train Loss: 303.63411872220536, Test Loss: 344.17373784493816\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 265, Train Loss: 302.7638043175404, Test Loss: 343.19766808420763\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 266, Train Loss: 301.89703272092345, Test Loss: 342.2253493978185\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 267, Train Loss: 301.03378951056766, Test Loss: 341.2567669689786\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 268, Train Loss: 300.1740603233934, Test Loss: 340.2919060402877\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 269, Train Loss: 299.3178308547891, Test Loss: 339.3307519134985\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 270, Train Loss: 298.4650868583734, Test Loss: 338.3732899492773\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 271, Train Loss: 297.6158141457578, Test Loss: 337.41950556696645\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 272, Train Loss: 296.7699985863109, Test Loss: 336.4693842443465\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 273, Train Loss: 295.9276261069232, Test Loss: 335.52291151740116\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 274, Train Loss: 295.08868269177265, Test Loss: 334.5800729800817\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 275, Train Loss: 294.2531543820919, Test Loss: 333.6408542840728\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 276, Train Loss: 293.421027275936, Test Loss: 332.7052411385598\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 277, Train Loss: 292.59228752795065, Test Loss: 331.7732193099962\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 278, Train Loss: 291.76692134914236, Test Loss: 330.84477462187203\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 279, Train Loss: 290.94491500664856, Test Loss: 329.91989295448434\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 280, Train Loss: 290.1262548235096, Test Loss: 328.99856024470705\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 281, Train Loss: 289.3109271784408, Test Loss: 328.0807624857629\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 282, Train Loss: 288.49891850560584, Test Loss: 327.1664857269957\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 283, Train Loss: 287.6902152943914, Test Loss: 326.25571607364384\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 284, Train Loss: 286.88480408918184, Test Loss: 325.3484396866144\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 285, Train Loss: 286.0826714891358, Test Loss: 324.4446427822586\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 286, Train Loss: 285.2838041479629, Test Loss: 323.5443116321477\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 287, Train Loss: 284.48818877370184, Test Loss: 322.64743256285\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 288, Train Loss: 283.6958121284991, Test Loss: 321.75399195570907\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 289, Train Loss: 282.9066610283889, Test Loss: 320.8639762466224\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 290, Train Loss: 282.1207223430737, Test Loss: 319.9773719258211\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 291, Train Loss: 281.3379829957058, Test Loss: 319.0941655376504\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 292, Train Loss: 280.55842996266944, Test Loss: 318.21434368035136\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 293, Train Loss: 279.7820502733648, Test Loss: 317.33789300584317\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 294, Train Loss: 279.00883100999135, Test Loss: 316.4648002195067\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 295, Train Loss: 278.23875930733374, Test Loss: 315.59505207996835\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 296, Train Loss: 277.47182235254695, Test Loss: 314.7286353988847\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 297, Train Loss: 276.7080073849438, Test Loss: 313.86553704072946\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 298, Train Loss: 275.9473016957821, Test Loss: 313.0057439225792\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 299, Train Loss: 275.1896926280534, Test Loss: 312.1492430139019\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 300, Train Loss: 274.43516757627276, Test Loss: 311.29602133634455\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 301, Train Loss: 273.6837139862683, Test Loss: 310.4460659635229\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 302, Train Loss: 272.93531935497265, Test Loss: 309.5993640208121\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 303, Train Loss: 272.1899712302154, Test Loss: 308.7559026851371\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 304, Train Loss: 271.447657210515, Test Loss: 307.9156691847646\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 305, Train Loss: 270.708364944873, Test Loss: 307.07865079909647\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 306, Train Loss: 269.9720821325685, Test Loss: 306.2448348584623\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 307, Train Loss: 269.2387965229536, Test Loss: 305.4142087439151\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 308, Train Loss: 268.508495915249, Test Loss: 304.58675988702515\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 309, Train Loss: 267.78116815834153, Test Loss: 303.7624757696773\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 310, Train Loss: 267.05680115058203, Test Loss: 302.94134392386707\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 311, Train Loss: 266.3353828395835, Test Loss: 302.12335193149875\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 312, Train Loss: 265.61690122202094, Test Loss: 301.30848742418397\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 313, Train Loss: 264.9013443434317, Test Loss: 300.49673808304107\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 314, Train Loss: 264.1887002980162, Test Loss: 299.68809163849505\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 315, Train Loss: 263.4789572284403, Test Loss: 298.8825358700788\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 316, Train Loss: 262.77210332563754, Test Loss: 298.0800586062349\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 317, Train Loss: 262.068126828613, Test Loss: 297.2806477241181\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 318, Train Loss: 261.36701602424756, Test Loss: 296.48429114939887\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 319, Train Loss: 260.6687592471029, Test Loss: 295.69097685606727\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 320, Train Loss: 259.9733448792272, Test Loss: 294.90069286623856\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 321, Train Loss: 259.28076134996263, Test Loss: 294.1134272499586\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 322, Train Loss: 258.59099713575193, Test Loss: 293.32916812501\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 323, Train Loss: 257.90404075994695, Test Loss: 292.5479036567205\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 324, Train Loss: 257.2198807926181, Test Loss: 291.76962205777033\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 325, Train Loss: 256.5385058503636, Test Loss: 290.9943115880012\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 326, Train Loss: 255.85990459612051, Test Loss: 290.2219605542262\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 327, Train Loss: 255.18406573897576, Test Loss: 289.45255731004045\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 328, Train Loss: 254.51097803397866, Test Loss: 288.6860902556318\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 329, Train Loss: 253.84063028195328, Test Loss: 287.92254783759324\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 330, Train Loss: 253.17301132931283, Test Loss: 287.16191854873523\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 331, Train Loss: 252.5081100678732, Test Loss: 286.40419092790006\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 332, Train Loss: 251.84591543466905, Test Loss: 285.649353559775\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 333, Train Loss: 251.18641641176893, Test Loss: 284.89739507470836\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 334, Train Loss: 250.52960202609273, Test Loss: 284.1483041485244\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 335, Train Loss: 249.87546134922826, Test Loss: 283.4020695023405\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 336, Train Loss: 249.22398349725015, Test Loss: 282.65867990238416\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 337, Train Loss: 248.57515763053834, Test Loss: 281.9181241598109\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 338, Train Loss: 247.928972953598, Test Loss: 281.18039113052316\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 339, Train Loss: 247.28541871487948, Test Loss: 280.44546971498994\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 340, Train Loss: 246.64448420660008, Test Loss: 279.7133488580665\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 341, Train Loss: 246.00615876456513, Test Loss: 278.98401754881587\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 342, Train Loss: 245.37043176799114, Test Loss: 278.2574648203301\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 343, Train Loss: 244.7372926393289, Test Loss: 277.53367974955273\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 344, Train Loss: 244.1067308440874, Test Loss: 276.8126514571022\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 345, Train Loss: 243.4787358906585, Test Loss: 276.0943691070953\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 346, Train Loss: 242.85329733014277, Test Loss: 275.3788219069717\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 347, Train Loss: 242.23040475617506, Test Loss: 274.66599910731964\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 348, Train Loss: 241.61004780475173, Test Loss: 273.9558900017014\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 349, Train Loss: 240.99221615405824, Test Loss: 273.24848392648005\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 350, Train Loss: 240.37689952429713, Test Loss: 272.54377026064714\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 351, Train Loss: 239.76408767751718, Test Loss: 271.84173842565036\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 352, Train Loss: 239.15377041744313, Test Loss: 271.1423778852226\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 353, Train Loss: 238.54593758930577, Test Loss: 270.445678145211\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 354, Train Loss: 237.94057907967317, Test Loss: 269.75162875340766\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 355, Train Loss: 237.33768481628246, Test Loss: 269.06021929938015\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 356, Train Loss: 236.73724476787203, Test Loss: 268.371439414303\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 357, Train Loss: 236.13924894401487, Test Loss: 267.6852787707897\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 358, Train Loss: 235.54368739495214, Test Loss: 267.00172708272635\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 359, Train Loss: 234.9505502114277, Test Loss: 266.3207741051042\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 360, Train Loss: 234.35982752452333, Test Loss: 265.64240963385475\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 361, Train Loss: 233.77150950549424, Test Loss: 264.9666235056841\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 362, Train Loss: 233.18558636560607, Test Loss: 264.2934055979086\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 363, Train Loss: 232.6020483559712, Test Loss: 263.6227458282915\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 364, Train Loss: 232.02088576738745, Test Loss: 262.9546341548795\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 365, Train Loss: 231.44208893017588, Test Loss: 262.28906057584004\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 366, Train Loss: 230.8656482140202, Test Loss: 261.62601512930013\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 367, Train Loss: 230.29155402780646, Test Loss: 260.96548789318484\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 368, Train Loss: 229.71979681946343, Test Loss: 260.307468985057\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 369, Train Loss: 229.1503670758037, Test Loss: 259.6519485619573\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 370, Train Loss: 228.58325532236546, Test Loss: 258.998916820245\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 371, Train Loss: 228.01845212325478, Test Loss: 258.34836399543946\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 372, Train Loss: 227.45594808098855, Test Loss: 257.70028036206264\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 373, Train Loss: 226.89573383633837, Test Loss: 257.0546562334813\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 374, Train Loss: 226.33780006817454, Test Loss: 256.4114819617507\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 375, Train Loss: 225.78213749331104, Test Loss: 255.77074793745845\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 376, Train Loss: 225.2287368663513, Test Loss: 255.13244458956927\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 377, Train Loss: 224.67758897953405, Test Loss: 254.49656238527047\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 378, Train Loss: 224.1286846625802, Test Loss: 253.8630918298172\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 379, Train Loss: 223.58201478254045, Test Loss: 253.23202346638004\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 380, Train Loss: 223.0375702436432, Test Loss: 252.6033478758911\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 381, Train Loss: 222.49534198714315, Test Loss: 251.97705567689246\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 382, Train Loss: 221.95532099117062, Test Loss: 251.35313752538434\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 383, Train Loss: 221.41749827058152, Test Loss: 250.731584114674\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 384, Train Loss: 220.8818648768077, Test Loss: 250.11238617522574\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 385, Train Loss: 220.34841189770816, Test Loss: 249.4955344745106\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 386, Train Loss: 219.81713045742077, Test Loss: 248.88101981685776\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 387, Train Loss: 219.28801171621473, Test Loss: 248.2688330433054\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 388, Train Loss: 218.76104687034302, Test Loss: 247.6589650314533\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 389, Train Loss: 218.23622715189634, Test Loss: 247.05140669531494\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 390, Train Loss: 217.71354382865727, Test Loss: 246.44614898517128\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 391, Train Loss: 217.19298820395466, Test Loss: 245.84318288742418\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 392, Train Loss: 216.6745516165191, Test Loss: 245.24249942445107\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 393, Train Loss: 216.15822544033892, Test Loss: 244.64408965445966\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 394, Train Loss: 215.64400108451636, Test Loss: 244.04794467134394\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 395, Train Loss: 215.13186999312495, Test Loss: 243.45405560454012\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 396, Train Loss: 214.62182364506708, Test Loss: 242.86241361888335\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 397, Train Loss: 214.11385355393188, Test Loss: 242.27300991446523\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 398, Train Loss: 213.60795126785462, Test Loss: 241.6858357264918\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 399, Train Loss: 213.10410836937558, Test Loss: 241.10088232514164\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 400, Train Loss: 212.6023164753002, Test Loss: 240.5181410154255\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 401, Train Loss: 212.10256723655965, Test Loss: 239.9376031370454\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 402, Train Loss: 211.6048523380718, Test Loss: 239.35926006425527\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 403, Train Loss: 211.10916349860287, Test Loss: 238.7831032057215\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 404, Train Loss: 210.61549247062965, Test Loss: 238.20912400438442\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 405, Train Loss: 210.12383104020248, Test Loss: 237.63731393732002\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 406, Train Loss: 209.63417102680822, Test Loss: 237.06766451560264\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 407, Train Loss: 209.1465042832344, Test Loss: 236.50016728416767\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 408, Train Loss: 208.66082269543355, Test Loss: 235.93481382167565\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 409, Train Loss: 208.17711818238826, Test Loss: 235.37159574037585\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 410, Train Loss: 207.69538269597683, Test Loss: 234.81050468597124\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 411, Train Loss: 207.21560822083902, Test Loss: 234.25153233748364\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 412, Train Loss: 206.73778677424286, Test Loss: 233.69467040711953\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 413, Train Loss: 206.26191040595214, Test Loss: 233.13991064013635\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 414, Train Loss: 205.7879711980936, Test Loss: 232.58724481470932\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 415, Train Loss: 205.31596126502552, Test Loss: 232.03666474179892\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 416, Train Loss: 204.84587275320652, Test Loss: 231.4881622650187\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 417, Train Loss: 204.37769784106464, Test Loss: 230.94172926050382\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 418, Train Loss: 203.91142873886741, Test Loss: 230.3973576367802\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 419, Train Loss: 203.44705768859245, Test Loss: 229.85503933463372\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 420, Train Loss: 202.9845769637977, Test Loss: 229.31476632698053\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 421, Train Loss: 202.52397886949362, Test Loss: 228.77653061873752\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 422, Train Loss: 202.0652557420147, Test Loss: 228.24032424669355\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 423, Train Loss: 201.60839994889216, Test Loss: 227.7061392793811\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 424, Train Loss: 201.15340388872684, Test Loss: 227.17396781694788\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 425, Train Loss: 200.70025999106264, Test Loss: 226.64380199103056\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 426, Train Loss: 200.2489607162609, Test Loss: 226.1156339646269\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 427, Train Loss: 199.7994985553747, Test Loss: 225.58945593197004\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 428, Train Loss: 199.3518660300237, Test Loss: 225.06526011840262\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 429, Train Loss: 198.90605569227034, Test Loss: 224.54303878025127\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 430, Train Loss: 198.46206012449528, Test Loss: 224.02278420470213\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 431, Train Loss: 198.01987193927428, Test Loss: 223.50448870967648\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 432, Train Loss: 197.5794837792554, Test Loss: 222.98814464370673\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 433, Train Loss: 197.14088831703629, Test Loss: 222.4737443858137\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 434, Train Loss: 196.70407825504248, Test Loss: 221.96128034538322\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 435, Train Loss: 196.26904632540584, Test Loss: 221.45074496204427\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 436, Train Loss: 195.83578528984373, Test Loss: 220.9421307055472\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 437, Train Loss: 195.40428793953865, Test Loss: 220.43543007564227\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 438, Train Loss: 194.97454709501804, Test Loss: 219.93063560195893\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 439, Train Loss: 194.54655560603513, Test Loss: 219.42773984388555\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 440, Train Loss: 194.1203063514496, Test Loss: 218.9267353904495\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 441, Train Loss: 193.69579223910958, Test Loss: 218.42761486019805\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 442, Train Loss: 193.2730062057332, Test Loss: 217.9303709010793\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 443, Train Loss: 192.85194121679126, Test Loss: 217.43499619032394\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 444, Train Loss: 192.43259026639026, Test Loss: 216.94148343432727\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 445, Train Loss: 192.01494637715564, Test Loss: 216.44982536853178\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 446, Train Loss: 191.59900260011585, Test Loss: 215.96001475731055\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 447, Train Loss: 191.18475201458676, Test Loss: 215.47204439385\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 448, Train Loss: 190.7721877280562, Test Loss: 214.98590710003512\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 449, Train Loss: 190.36130287606974, Test Loss: 214.5015957263327\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 450, Train Loss: 189.95209062211606, Test Loss: 214.019103151677\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 451, Train Loss: 189.54454415751346, Test Loss: 213.53842228335503\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 452, Train Loss: 189.13865670129647, Test Loss: 213.05954605689232\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 453, Train Loss: 188.73442150010302, Test Loss: 212.5824674359394\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 454, Train Loss: 188.3318318280622, Test Loss: 212.10717941215847\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 455, Train Loss: 187.9308809866821, Test Loss: 211.6336750051109\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 456, Train Loss: 187.5315623047387, Test Loss: 211.16194726214445\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 457, Train Loss: 187.13386913816453, Test Loss: 210.69198925828192\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 458, Train Loss: 186.73779486993823, Test Loss: 210.2237940961097\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 459, Train Loss: 186.3433329099746, Test Loss: 209.75735490566663\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 460, Train Loss: 185.9504766950149, Test Loss: 209.29266484433367\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 461, Train Loss: 185.55921968851743, Test Loss: 208.8297170967238\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 462, Train Loss: 185.16955538054913, Test Loss: 208.3685048745725\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 463, Train Loss: 184.7814772876768, Test Loss: 207.90902141662866\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 464, Train Loss: 184.39497895285973, Test Loss: 207.45125998854553\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 465, Train Loss: 184.01005394534198, Test Loss: 206.99521388277319\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 466, Train Loss: 183.62669586054534, Test Loss: 206.54087641844978\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 467, Train Loss: 183.24489831996294, Test Loss: 206.08824094129488\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 468, Train Loss: 182.86465497105303, Test Loss: 205.6373008235022\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 469, Train Loss: 182.4859594871333, Test Loss: 205.18804946363315\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 470, Train Loss: 182.1088055672755, Test Loss: 204.74048028651072\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 471, Train Loss: 181.7331869362008, Test Loss: 204.29458674311383\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 472, Train Loss: 181.3590973441754, Test Loss: 203.85036231047212\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 473, Train Loss: 180.98653056690617, Test Loss: 203.40780049156115\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 474, Train Loss: 180.61548040543755, Test Loss: 202.96689481519803\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 475, Train Loss: 180.2459406860481, Test Loss: 202.52763883593735\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 476, Train Loss: 179.8779052601479, Test Loss: 202.0900261339678\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 477, Train Loss: 179.51136800417635, Test Loss: 201.65405031500887\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 478, Train Loss: 179.14632281949997, Test Loss: 201.21970501020843\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 479, Train Loss: 178.7827636323113, Test Loss: 200.7869838760401\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 480, Train Loss: 178.4206843935275, Test Loss: 200.3558805942016\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 481, Train Loss: 178.06007907868997, Test Loss: 199.92638887151332\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 482, Train Loss: 177.70094168786403, Test Loss: 199.498502439817\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 483, Train Loss: 177.34326624553893, Test Loss: 199.0722150558755\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 484, Train Loss: 176.98704680052873, Test Loss: 198.6475205012721\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 485, Train Loss: 176.63227742587299, Test Loss: 198.22441258231115\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 486, Train Loss: 176.27895221873834, Test Loss: 197.80288512991814\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 487, Train Loss: 175.92706530032012, Test Loss: 197.3829319995413\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 488, Train Loss: 175.57661081574486, Test Loss: 196.96454707105227\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 489, Train Loss: 175.22758293397243, Test Loss: 196.54772424864842\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 490, Train Loss: 174.87997584769926, Test Loss: 196.13245746075484\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 491, Train Loss: 174.53378377326192, Test Loss: 195.71874065992682\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 492, Train Loss: 174.1890009505404, Test Loss: 195.30656782275298\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 493, Train Loss: 173.84562164286257, Test Loss: 194.8959329497585\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 494, Train Loss: 173.50364013690898, Test Loss: 194.48683006530905\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 495, Train Loss: 173.16305074261717, Test Loss: 194.07925321751452\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 496, Train Loss: 172.82384779308748, Test Loss: 193.6731964781341\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 497, Train Loss: 172.48602564448862, Test Loss: 193.26865394248082\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 498, Train Loss: 172.1495786759639, Test Loss: 192.86561972932685\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 499, Train Loss: 171.8145012895373, Test Loss: 192.46408798080944\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 500, Train Loss: 171.48078791002087, Test Loss: 192.06405286233667\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 501, Train Loss: 171.1484329849215, Test Loss: 191.6655085624941\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 502, Train Loss: 170.81743098434882, Test Loss: 191.26844929295152\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 503, Train Loss: 170.48777640092317, Test Loss: 190.8728692883701\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 504, Train Loss: 170.15946374968382, Test Loss: 190.47876280631016\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 505, Train Loss: 169.83248756799782, Test Loss: 190.08612412713876\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 506, Train Loss: 169.506842415469, Test Loss: 189.6949475539384\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 507, Train Loss: 169.18252287384766, Test Loss: 189.3052274124155\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 508, Train Loss: 168.85952354694024, Test Loss: 188.9169580508094\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 509, Train Loss: 168.53783906051956, Test Loss: 188.5301338398019\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 510, Train Loss: 168.21746406223542, Test Loss: 188.14474917242723\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 511, Train Loss: 167.89839322152565, Test Loss: 187.76079846398173\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 512, Train Loss: 167.58062122952708, Test Loss: 187.37827615193487\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 513, Train Loss: 167.2641427989878, Test Loss: 186.99717669583987\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 514, Train Loss: 166.9489526641785, Test Loss: 186.61749457724488\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 515, Train Loss: 166.63504558080535, Test Loss: 186.23922429960473\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 516, Train Loss: 166.32241632592257, Test Loss: 185.86236038819294\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 517, Train Loss: 166.01105969784552, Test Loss: 185.48689739001395\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 518, Train Loss: 165.70097051606422, Test Loss: 185.11282987371564\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 519, Train Loss: 165.39214362115703, Test Loss: 184.7401524295027\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 520, Train Loss: 165.08457387470494, Test Loss: 184.3688596690496\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 521, Train Loss: 164.77825615920597, Test Loss: 183.99894622541473\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 522, Train Loss: 164.47318537799012, Test Loss: 183.63040675295412\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 523, Train Loss: 164.16935645513456, Test Loss: 183.263235927236\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 524, Train Loss: 163.86676433537892, Test Loss: 182.89742844495575\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 525, Train Loss: 163.56540398404158, Test Loss: 182.53297902385066\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 526, Train Loss: 163.2652703869357, Test Loss: 182.16988240261566\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 527, Train Loss: 162.96635855028575, Test Loss: 181.80813334081898\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 528, Train Loss: 162.66866350064447, Test Loss: 181.4477266188184\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 529, Train Loss: 162.37218028481016, Test Loss: 181.0886570376776\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 530, Train Loss: 162.07690396974417, Test Loss: 180.73091941908308\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 531, Train Loss: 161.78282964248893, Test Loss: 180.37450860526133\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 532, Train Loss: 161.48995241008618, Test Loss: 180.01941945889595\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 533, Train Loss: 161.19826739949542, Test Loss: 179.66564686304622\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 534, Train Loss: 160.90776975751305, Test Loss: 179.31318572106454\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 535, Train Loss: 160.61845465069146, Test Loss: 178.96203095651524\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 536, Train Loss: 160.3303172652587, Test Loss: 178.61217751309334\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 537, Train Loss: 160.04335280703833, Test Loss: 178.26362035454372\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 538, Train Loss: 159.7575565013697, Test Loss: 177.91635446458048\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 539, Train Loss: 159.47292359302833, Test Loss: 177.5703748468069\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 540, Train Loss: 159.18944934614711, Test Loss: 177.22567652463547\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 541, Train Loss: 158.90712904413732, Test Loss: 176.88225454120828\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 542, Train Loss: 158.62595798961007, Test Loss: 176.54010395931783\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 543, Train Loss: 158.34593150429828, Test Loss: 176.1992198613282\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 544, Train Loss: 158.0670449289788, Test Loss: 175.8595973490962\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 545, Train Loss: 157.7892936233948, Test Loss: 175.52123154389355\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 546, Train Loss: 157.51267296617883, Test Loss: 175.18411758632826\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 547, Train Loss: 157.23717835477555, Test Loss: 174.84825063626778\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 548, Train Loss: 156.9628052053655, Test Loss: 174.51362587276083\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 549, Train Loss: 156.68954895278853, Test Loss: 174.18023849396107\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 550, Train Loss: 156.41740505046806, Test Loss: 173.8480837170499\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 551, Train Loss: 156.1463689703354, Test Loss: 173.51715677816054\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 552, Train Loss: 155.87643620275435, Test Loss: 173.1874529323013\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 553, Train Loss: 155.6076022564461, Test Loss: 172.85896745328046\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 554, Train Loss: 155.33986265841466, Test Loss: 172.53169563363033\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 555, Train Loss: 155.07321295387237, Test Loss: 172.20563278453238\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 556, Train Loss: 154.80764870616576, Test Loss: 171.88077423574205\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 557, Train Loss: 154.54316549670176, Test Loss: 171.55711533551448\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 558, Train Loss: 154.27975892487405, Test Loss: 171.2346514505303\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 559, Train Loss: 154.01742460799016, Test Loss: 170.9133779658211\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 560, Train Loss: 153.7561581811982, Test Loss: 170.59329028469645\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 561, Train Loss: 153.49595529741435, Test Loss: 170.27438382867024\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 562, Train Loss: 153.23681162725066, Test Loss: 169.95665403738758\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 563, Train Loss: 152.97872285894294, Test Loss: 169.64009636855192\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 564, Train Loss: 152.7216846982788, Test Loss: 169.32470629785303\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 565, Train Loss: 152.4656928685267, Test Loss: 169.0104793188941\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 566, Train Loss: 152.21074311036415, Test Loss: 168.69741094312053\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 567, Train Loss: 151.9568311818074, Test Loss: 168.38549669974793\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 568, Train Loss: 151.70395285814055, Test Loss: 168.0747321356909\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 569, Train Loss: 151.4521039318454, Test Loss: 167.765112815492\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 570, Train Loss: 151.20128021253132, Test Loss: 167.4566343212511\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 571, Train Loss: 150.95147752686572, Test Loss: 167.14929225255463\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 572, Train Loss: 150.7026917185043, Test Loss: 166.8430822264058\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 573, Train Loss: 150.45491864802221, Test Loss: 166.5379998771545\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 574, Train Loss: 150.20815419284503, Test Loss: 166.23404085642773\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 575, Train Loss: 149.96239424718016, Test Loss: 165.93120083306025\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 576, Train Loss: 149.71763472194866, Test Loss: 165.62947549302547\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 577, Train Loss: 149.47387154471687, Test Loss: 165.32886053936696\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 578, Train Loss: 149.23110065962902, Test Loss: 165.02935169212964\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 579, Train Loss: 148.9893180273397, Test Loss: 164.7309446882918\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 580, Train Loss: 148.74851962494634, Test Loss: 164.43363528169695\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 581, Train Loss: 148.50870144592258, Test Loss: 164.13741924298643\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 582, Train Loss: 148.26985950005155, Test Loss: 163.84229235953183\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 583, Train Loss: 148.03198981335947, Test Loss: 163.5482504353678\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 584, Train Loss: 147.79508842804933, Test Loss: 163.25528929112556\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 585, Train Loss: 147.55915140243542, Test Loss: 162.96340476396594\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 586, Train Loss: 147.32417481087742, Test Loss: 162.6725927075133\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 587, Train Loss: 147.09015474371526, Test Loss: 162.38284899178944\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 588, Train Loss: 146.85708730720393, Test Loss: 162.09416950314773\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 589, Train Loss: 146.6249686234488, Test Loss: 161.80655014420788\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 590, Train Loss: 146.39379483034116, Test Loss: 161.51998683379028\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 591, Train Loss: 146.16356208149375, Test Loss: 161.2344755068515\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 592, Train Loss: 145.93426654617693, Test Loss: 160.95001211441928\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 593, Train Loss: 145.705904409255, Test Loss: 160.66659262352806\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 594, Train Loss: 145.47847187112245, Test Loss: 160.38421301715493\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 595, Train Loss: 145.25196514764093, Test Loss: 160.1028692941556\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 596, Train Loss: 145.0263804700764, Test Loss: 159.82255746920083\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 597, Train Loss: 144.8017140850361, Test Loss: 159.54327357271293\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 598, Train Loss: 144.57796225440646, Test Loss: 159.26501365080264\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 599, Train Loss: 144.35512125529064, Test Loss: 158.9877737652063\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 600, Train Loss: 144.13318737994678, Test Loss: 158.71154999322283\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 601, Train Loss: 143.91215693572602, Test Loss: 158.43633842765195\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 602, Train Loss: 143.6920262450114, Test Loss: 158.16213517673148\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 603, Train Loss: 143.47279164515646, Test Loss: 157.88893636407565\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 604, Train Loss: 143.25444948842434, Test Loss: 157.61673812861363\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 605, Train Loss: 143.03699614192715, Test Loss: 157.34553662452794\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 606, Train Loss: 142.82042798756552, Test Loss: 157.07532802119317\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 607, Train Loss: 142.60474142196819, Test Loss: 156.80610850311552\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 608, Train Loss: 142.3899328564324, Test Loss: 156.5378742698716\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 609, Train Loss: 142.17599871686394, Test Loss: 156.27062153604845\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 610, Train Loss: 141.96293544371764, Test Loss: 156.0043465311832\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 611, Train Loss: 141.7507394919384, Test Loss: 155.7390454997031\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 612, Train Loss: 141.53940733090195, Test Loss: 155.4747147008661\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 613, Train Loss: 141.32893544435632, Test Loss: 155.21135040870098\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 614, Train Loss: 141.11932033036317, Test Loss: 154.9489489119485\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 615, Train Loss: 140.91055850123948, Test Loss: 154.68750651400237\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 616, Train Loss: 140.70264648349976, Test Loss: 154.42701953285035\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 617, Train Loss: 140.49558081779804, Test Loss: 154.1674843010161\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 618, Train Loss: 140.2893580588704, Test Loss: 153.9088971655006\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 619, Train Loss: 140.08397477547766, Test Loss: 153.65125448772432\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 620, Train Loss: 139.87942755034825, Test Loss: 153.39455264346935\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 621, Train Loss: 139.67571298012132, Test Loss: 153.13878802282196\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 622, Train Loss: 139.47282767529018, Test Loss: 152.8839570301152\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 623, Train Loss: 139.27076826014596, Test Loss: 152.63005608387172\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 624, Train Loss: 139.06953137272117, Test Loss: 152.37708161674738\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 625, Train Loss: 138.8691136647342, Test Loss: 152.12503007547383\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 626, Train Loss: 138.66951180153308, Test Loss: 151.873897920803\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 627, Train Loss: 138.47072246204056, Test Loss: 151.6236816274502\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 628, Train Loss: 138.27274233869844, Test Loss: 151.37437768403876\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 629, Train Loss: 138.0755681374127, Test Loss: 151.12598259304377\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 630, Train Loss: 137.87919657749856, Test Loss: 150.87849287073715\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 631, Train Loss: 137.68362439162615, Test Loss: 150.63190504713188\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 632, Train Loss: 137.4888483257659, Test Loss: 150.38621566592732\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 633, Train Loss: 137.29486513913452, Test Loss: 150.14142128445428\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 634, Train Loss: 137.10167160414105, Test Loss: 149.89751847362027\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 635, Train Loss: 136.9092645063331, Test Loss: 149.65450381785536\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 636, Train Loss: 136.71764064434348, Test Loss: 149.4123739150579\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 637, Train Loss: 136.52679682983685, Test Loss: 149.17112537654066\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 638, Train Loss: 136.33672988745678, Test Loss: 148.93075482697697\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 639, Train Loss: 136.14743665477272, Test Loss: 148.69125890434725\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 640, Train Loss: 135.9589139822275, Test Loss: 148.45263425988594\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 641, Train Loss: 135.77115873308503, Test Loss: 148.21487755802806\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 642, Train Loss: 135.5841677833779, Test Loss: 147.97798547635676\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 643, Train Loss: 135.39793802185548, Test Loss: 147.74195470555026\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 644, Train Loss: 135.21246634993233, Test Loss: 147.5067819493298\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 645, Train Loss: 135.02774968163632, Test Loss: 147.2724639244071\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 646, Train Loss: 134.84378494355758, Test Loss: 147.03899736043257\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 647, Train Loss: 134.6605690747972, Test Loss: 146.80637899994343\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 648, Train Loss: 134.4780990269162, Test Loss: 146.57460559831202\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 649, Train Loss: 134.2963717638852, Test Loss: 146.34367392369472\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 650, Train Loss: 134.11538426203344, Test Loss: 146.1135807569804\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 651, Train Loss: 133.93513350999882, Test Loss: 145.8843228917396\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 652, Train Loss: 133.75561650867758, Test Loss: 145.65589713417396\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 653, Train Loss: 133.57683027117452, Test Loss: 145.42830030306536\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 654, Train Loss: 133.3987718227532, Test Loss: 145.20152922972574\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 655, Train Loss: 133.22143820078657, Test Loss: 144.9755807579469\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 656, Train Loss: 133.0448264547076, Test Loss: 144.75045174395058\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 657, Train Loss: 132.86893364596017, Test Loss: 144.52613905633865\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 658, Train Loss: 132.69375684795023, Test Loss: 144.3026395760437\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 659, Train Loss: 132.51929314599707, Test Loss: 144.07995019627947\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 660, Train Loss: 132.34553963728487, Test Loss: 143.85806782249185\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 661, Train Loss: 132.17249343081428, Test Loss: 143.63698937230998\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 662, Train Loss: 132.00015164735447, Test Loss: 143.41671177549753\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 663, Train Loss: 131.8285114193952, Test Loss: 143.19723197390383\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 664, Train Loss: 131.65756989109894, Test Loss: 142.97854692141584\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 665, Train Loss: 131.4873242182536, Test Loss: 142.76065358390989\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 666, Train Loss: 131.31777156822503, Test Loss: 142.54354893920362\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 667, Train Loss: 131.14890911990994, Test Loss: 142.3272299770084\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 668, Train Loss: 130.980734063689, Test Loss: 142.11169369888157\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 669, Train Loss: 130.8132436013801, Test Loss: 141.89693711817915\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 670, Train Loss: 130.64643494619165, Test Loss: 141.6829572600085\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 671, Train Loss: 130.48030532267637, Test Loss: 141.46975116118148\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 672, Train Loss: 130.314851966685, Test Loss: 141.2573158701676\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 673, Train Loss: 130.15007212532046, Test Loss: 141.04564844704723\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 674, Train Loss: 129.98596305689188, Test Loss: 140.83474596346542\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 675, Train Loss: 129.8225220308691, Test Loss: 140.62460550258544\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 676, Train Loss: 129.65974632783713, Test Loss: 140.41522415904274\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 677, Train Loss: 129.49763323945095, Test Loss: 140.20659903889918\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 678, Train Loss: 129.33618006839058, Test Loss: 139.99872725959727\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 679, Train Loss: 129.175384128316, Test Loss: 139.79160594991453\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 680, Train Loss: 129.01524274382245, Test Loss: 139.58523224991836\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 681, Train Loss: 128.8557532503962, Test Loss: 139.37960331092074\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 682, Train Loss: 128.6969129943698, Test Loss: 139.17471629543346\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 683, Train Loss: 128.53871933287826, Test Loss: 138.97056837712307\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 684, Train Loss: 128.38116963381495, Test Loss: 138.76715674076658\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 685, Train Loss: 128.22426127578785, Test Loss: 138.56447858220673\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 686, Train Loss: 128.0679916480758, Test Loss: 138.3625311083079\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 687, Train Loss: 127.91235815058526, Test Loss: 138.1613115369121\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 688, Train Loss: 127.75735819380682, Test Loss: 137.9608170967951\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 689, Train Loss: 127.60298919877242, Test Loss: 137.76104502762246\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 690, Train Loss: 127.44924859701212, Test Loss: 137.56199257990633\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 691, Train Loss: 127.29613383051154, Test Loss: 137.36365701496175\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 692, Train Loss: 127.14364235166927, Test Loss: 137.16603560486396\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 693, Train Loss: 126.99177162325458, Test Loss: 136.96912563240483\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 694, Train Loss: 126.84051911836487, Test Loss: 136.77292439105034\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 695, Train Loss: 126.68988232038413, Test Loss: 136.57742918489794\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 696, Train Loss: 126.53985872294058, Test Loss: 136.38263732863385\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 697, Train Loss: 126.39044582986523, Test Loss: 136.18854614749105\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 698, Train Loss: 126.24164115515045, Test Loss: 135.99515297720683\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 699, Train Loss: 126.09344222290824, Test Loss: 135.80245516398094\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 700, Train Loss: 125.94584656732944, Test Loss: 135.6104500644338\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 701, Train Loss: 125.79885173264233, Test Loss: 135.41913504556473\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 702, Train Loss: 125.65245527307209, Test Loss: 135.2285074847108\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 703, Train Loss: 125.50665475279993, Test Loss: 135.03856476950514\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 704, Train Loss: 125.36144774592265, Test Loss: 134.84930429783594\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 705, Train Loss: 125.21683183641207, Test Loss: 134.66072347780562\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 706, Train Loss: 125.07280461807517, Test Loss: 134.47281972768988\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 707, Train Loss: 124.92936369451382, Test Loss: 134.28559047589698\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 708, Train Loss: 124.78650667908491, Test Loss: 134.09903316092743\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 709, Train Loss: 124.64423119486068, Test Loss: 133.91314523133371\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 710, Train Loss: 124.5025348745893, Test Loss: 133.72792414567976\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 711, Train Loss: 124.36141536065523, Test Loss: 133.5433673725015\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 712, Train Loss: 124.22087030504025, Test Loss: 133.35947239026663\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 713, Train Loss: 124.08089736928414, Test Loss: 133.17623668733498\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 714, Train Loss: 123.94149422444599, Test Loss: 132.99365776191922\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 715, Train Loss: 123.80265855106533, Test Loss: 132.81173312204535\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 716, Train Loss: 123.6643880391236, Test Loss: 132.63046028551352\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 717, Train Loss: 123.52668038800564, Test Loss: 132.449836779859\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 718, Train Loss: 123.3895333064615, Test Loss: 132.26986014231335\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 719, Train Loss: 123.2529445125682, Test Loss: 132.09052791976558\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 720, Train Loss: 123.11691173369194, Test Loss: 131.91183766872382\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 721, Train Loss: 122.98143270645008, Test Loss: 131.73378695527663\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 722, Train Loss: 122.84650517667365, Test Loss: 131.55637335505492\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 723, Train Loss: 122.71212689936972, Test Loss: 131.37959445319387\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 724, Train Loss: 122.5782956386842, Test Loss: 131.20344784429483\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 725, Train Loss: 122.44500916786436, Test Loss: 131.02793113238766\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 726, Train Loss: 122.31226526922217, Test Loss: 130.85304193089308\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 727, Train Loss: 122.18006173409708, Test Loss: 130.67877786258506\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 728, Train Loss: 122.04839636281935, Test Loss: 130.50513655955373\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 729, Train Loss: 121.91726696467359, Test Loss: 130.33211566316788\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 730, Train Loss: 121.78667135786208, Test Loss: 130.15971282403817\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 731, Train Loss: 121.65660736946874, Test Loss: 129.98792570198006\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 732, Train Loss: 121.52707283542269, Test Loss: 129.8167519659773\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 733, Train Loss: 121.39806560046242, Test Loss: 129.6461892941451\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 734, Train Loss: 121.2695835180999, Test Loss: 129.47623537369378\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 735, Train Loss: 121.14162445058497, Test Loss: 129.30688790089255\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 736, Train Loss: 121.01418626886945, Test Loss: 129.13814458103323\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 737, Train Loss: 120.88726685257203, Test Loss: 128.97000312839432\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 738, Train Loss: 120.76086408994294, Test Loss: 128.8024612662051\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 739, Train Loss: 120.6349758778286, Test Loss: 128.63551672661\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 740, Train Loss: 120.50960012163696, Test Loss: 128.46916725063292\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 741, Train Loss: 120.38473473530235, Test Loss: 128.30341058814187\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 742, Train Loss: 120.26037764125094, Test Loss: 128.13824449781362\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 743, Train Loss: 120.13652677036613, Test Loss: 127.9736667470986\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 744, Train Loss: 120.0131800619541, Test Loss: 127.80967511218594\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 745, Train Loss: 119.8903354637095, Test Loss: 127.64626737796856\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 746, Train Loss: 119.7679909316815, Test Loss: 127.48344133800842\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 747, Train Loss: 119.64614443023937, Test Loss: 127.32119479450198\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 748, Train Loss: 119.5247939320391, Test Loss: 127.15952555824583\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 749, Train Loss: 119.4039374179894, Test Loss: 126.99843144860225\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 750, Train Loss: 119.28357287721799, Test Loss: 126.83791029346507\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 751, Train Loss: 119.16369830703847, Test Loss: 126.67795992922578\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 752, Train Loss: 119.04431171291671, Test Loss: 126.51857820073948\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 753, Train Loss: 118.9254111084378, Test Loss: 126.35976296129108\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 754, Train Loss: 118.80699451527295, Test Loss: 126.20151207256208\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 755, Train Loss: 118.68905996314666, Test Loss: 126.04382340459657\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 756, Train Loss: 118.57160548980389, Test Loss: 125.88669483576811\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 757, Train Loss: 118.4546291409772, Test Loss: 125.73012425274663\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 758, Train Loss: 118.33812897035467, Test Loss: 125.57410955046527\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 759, Train Loss: 118.22210303954722, Test Loss: 125.41864863208727\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 760, Train Loss: 118.10654941805628, Test Loss: 125.26373940897338\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 761, Train Loss: 117.99146618324195, Test Loss: 125.1093798006491\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 762, Train Loss: 117.8768514202908, Test Loss: 124.9555677347721\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 763, Train Loss: 117.76270322218402, Test Loss: 124.80230114709985\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 764, Train Loss: 117.64901968966589, Test Loss: 124.6495779814573\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 765, Train Loss: 117.53579893121193, Test Loss: 124.49739618970472\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 766, Train Loss: 117.42303906299747, Test Loss: 124.34575373170574\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 767, Train Loss: 117.31073820886655, Test Loss: 124.19464857529539\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 768, Train Loss: 117.19889450030036, Test Loss: 124.04407869624823\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 769, Train Loss: 117.08750607638636, Test Loss: 123.89404207824698\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 770, Train Loss: 116.97657108378726, Test Loss: 123.74453671285073\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 771, Train Loss: 116.86608767671021, Test Loss: 123.59556059946367\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 772, Train Loss: 116.75605401687606, Test Loss: 123.44711174530391\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 773, Train Loss: 116.64646827348881, Test Loss: 123.29918816537216\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 774, Train Loss: 116.53732862320507, Test Loss: 123.15178788242088\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 775, Train Loss: 116.42863325010379, Test Loss: 123.00490892692332\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 776, Train Loss: 116.32038034565609, Test Loss: 122.85854933704277\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 777, Train Loss: 116.21256810869502, Test Loss: 122.71270715860192\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 778, Train Loss: 116.10519474538575, Test Loss: 122.56738044505232\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 779, Train Loss: 115.9982584691956, Test Loss: 122.42256725744406\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 780, Train Loss: 115.89175750086437, Test Loss: 122.27826566439543\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 781, Train Loss: 115.78569006837479, Test Loss: 122.13447374206285\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 782, Train Loss: 115.68005440692293, Test Loss: 121.99118957411076\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 783, Train Loss: 115.5748487588889, Test Loss: 121.84841125168181\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 784, Train Loss: 115.4700713738076, Test Loss: 121.70613687336692\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 785, Train Loss: 115.36572050833954, Test Loss: 121.5643645451758\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 786, Train Loss: 115.26179442624198, Test Loss: 121.42309238050736\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 787, Train Loss: 115.15829139833988, Test Loss: 121.28231850012011\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 788, Train Loss: 115.05520970249714, Test Loss: 121.14204103210305\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 789, Train Loss: 114.95254762358806, Test Loss: 121.00225811184649\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 790, Train Loss: 114.85030345346874, Test Loss: 120.86296788201284\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 791, Train Loss: 114.74847549094869, Test Loss: 120.72416849250772\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 792, Train Loss: 114.64706204176235, Test Loss: 120.58585810045116\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 793, Train Loss: 114.54606141854126, Test Loss: 120.44803487014886\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 794, Train Loss: 114.44547194078564, Test Loss: 120.31069697306344\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 795, Train Loss: 114.34529193483647, Test Loss: 120.17384258778625\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 796, Train Loss: 114.24551973384794, Test Loss: 120.03746990000874\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 797, Train Loss: 114.14615367775934, Test Loss: 119.90157710249423\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 798, Train Loss: 114.04719211326771, Test Loss: 119.76616239504986\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 799, Train Loss: 113.94863339380014, Test Loss: 119.63122398449849\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 800, Train Loss: 113.85047587948651, Test Loss: 119.49676008465077\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 801, Train Loss: 113.75271793713219, Test Loss: 119.36276891627739\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 802, Train Loss: 113.6553579401907, Test Loss: 119.22924870708133\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 803, Train Loss: 113.5583942687369, Test Loss: 119.09619769167023\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 804, Train Loss: 113.46182530943997, Test Loss: 118.96361411152907\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 805, Train Loss: 113.36564945553633, Test Loss: 118.83149621499264\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 806, Train Loss: 113.26986510680325, Test Loss: 118.69984225721831\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 807, Train Loss: 113.17447066953193, Test Loss: 118.56865050015897\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 808, Train Loss: 113.07946455650122, Test Loss: 118.43791921253589\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 809, Train Loss: 112.98484518695105, Test Loss: 118.30764666981193\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 810, Train Loss: 112.89061098655615, Test Loss: 118.17783115416464\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 811, Train Loss: 112.79676038740007, Test Loss: 118.04847095445943\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 812, Train Loss: 112.70329182794868, Test Loss: 117.9195643662232\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 813, Train Loss: 112.61020375302456, Test Loss: 117.79110969161763\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 814, Train Loss: 112.51749461378091, Test Loss: 117.66310523941297\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 815, Train Loss: 112.42516286767592, Test Loss: 117.53554932496169\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 816, Train Loss: 112.33320697844704, Test Loss: 117.40844027017215\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 817, Train Loss: 112.24162541608536, Test Loss: 117.28177640348278\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 818, Train Loss: 112.15041665681025, Test Loss: 117.15555605983599\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 819, Train Loss: 112.05957918304401, Test Loss: 117.02977758065214\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 820, Train Loss: 111.96911148338643, Test Loss: 116.90443931380423\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 821, Train Loss: 111.87901205258991, Test Loss: 116.77953961359177\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 822, Train Loss: 111.7892793915344, Test Loss: 116.65507684071544\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 823, Train Loss: 111.69991200720202, Test Loss: 116.53104936225161\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 824, Train Loss: 111.61090841265289, Test Loss: 116.40745555162704\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 825, Train Loss: 111.52226712699985, Test Loss: 116.28429378859367\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 826, Train Loss: 111.43398667538413, Test Loss: 116.16156245920327\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 827, Train Loss: 111.34606558895065, Test Loss: 116.03925995578257\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 828, Train Loss: 111.25850240482362, Test Loss: 115.91738467690831\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 829, Train Loss: 111.17129566608219, Test Loss: 115.79593502738234\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 830, Train Loss: 111.08444392173631, Test Loss: 115.67490941820688\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 831, Train Loss: 110.99794572670247, Test Loss: 115.55430626655996\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 832, Train Loss: 110.91179964177965, Test Loss: 115.4341239957707\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 833, Train Loss: 110.82600423362544, Test Loss: 115.3143610352951\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 834, Train Loss: 110.74055807473218, Test Loss: 115.19501582069162\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 835, Train Loss: 110.65545974340327, Test Loss: 115.07608679359669\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 836, Train Loss: 110.57070782372932, Test Loss: 114.9575724017009\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 837, Train Loss: 110.48630090556478, Test Loss: 114.8394710987248\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 838, Train Loss: 110.40223758450435, Test Loss: 114.72178134439483\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 839, Train Loss: 110.31851646185973, Test Loss: 114.6045016044197\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 840, Train Loss: 110.23513614463631, Test Loss: 114.48763035046645\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 841, Train Loss: 110.15209524550993, Test Loss: 114.37116606013682\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 842, Train Loss: 110.06939238280383, Test Loss: 114.25510721694366\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 843, Train Loss: 109.9870261804657, Test Loss: 114.13945231028757\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 844, Train Loss: 109.90499526804479, Test Loss: 114.02419983543328\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 845, Train Loss: 109.82329828066904, Test Loss: 113.90934829348663\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 846, Train Loss: 109.74193385902237, Test Loss: 113.79489619137112\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 847, Train Loss: 109.66090064932209, Test Loss: 113.68084204180502\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 848, Train Loss: 109.58019730329649, Test Loss: 113.56718436327822\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 849, Train Loss: 109.49982247816222, Test Loss: 113.45392168002931\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 850, Train Loss: 109.419774836602, Test Loss: 113.34105252202289\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 851, Train Loss: 109.34005304674247, Test Loss: 113.22857542492673\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 852, Train Loss: 109.26065578213192, Test Loss: 113.11648893008905\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 853, Train Loss: 109.18158172171823, Test Loss: 113.00479158451614\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 854, Train Loss: 109.10282954982696, Test Loss: 112.89348194084982\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 855, Train Loss: 109.02439795613942, Test Loss: 112.782558557345\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 856, Train Loss: 108.94628563567085, Test Loss: 112.67201999784751\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 857, Train Loss: 108.86849128874866, Test Loss: 112.56186483177184\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 858, Train Loss: 108.79101362099095, Test Loss: 112.45209163407903\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 859, Train Loss: 108.71385134328486, Test Loss: 112.34269898525471\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 860, Train Loss: 108.63700317176516, Test Loss: 112.23368547128717\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 861, Train Loss: 108.56046782779282, Test Loss: 112.12504968364549\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 862, Train Loss: 108.48424403793386, Test Loss: 112.01679021925779\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 863, Train Loss: 108.40833053393816, Test Loss: 111.90890568048951\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 864, Train Loss: 108.33272605271812, Test Loss: 111.801394675122\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 865, Train Loss: 108.25742933632793, Test Loss: 111.69425581633081\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 866, Train Loss: 108.18243913194249, Test Loss: 111.5874877226645\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 867, Train Loss: 108.10775419183662, Test Loss: 111.48108901802325\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 868, Train Loss: 108.0333732733643, Test Loss: 111.3750583316376\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 869, Train Loss: 107.95929513893797, Test Loss: 111.26939429804723\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 870, Train Loss: 107.88551855600792, Test Loss: 111.16409555708003\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 871, Train Loss: 107.81204229704181, Test Loss: 111.05916075383118\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 872, Train Loss: 107.73886513950426, Test Loss: 110.95458853864196\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 873, Train Loss: 107.66598586583649, Test Loss: 110.85037756707928\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 874, Train Loss: 107.59340326343609, Test Loss: 110.74652649991481\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 875, Train Loss: 107.52111612463672, Test Loss: 110.64303400310423\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 876, Train Loss: 107.4491232466882, Test Loss: 110.53989874776698\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 877, Train Loss: 107.37742343173645, Test Loss: 110.4371194101654\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 878, Train Loss: 107.30601548680337, Test Loss: 110.33469467168462\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 879, Train Loss: 107.23489822376733, Test Loss: 110.23262321881211\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 880, Train Loss: 107.16407045934304, Test Loss: 110.13090374311754\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 881, Train Loss: 107.09353101506213, Test Loss: 110.02953494123258\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 882, Train Loss: 107.02327871725345, Test Loss: 109.92851551483085\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 883, Train Loss: 106.95331239702354, Test Loss: 109.82784417060786\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 884, Train Loss: 106.88363089023713, Test Loss: 109.72751962026118\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 885, Train Loss: 106.81423303749781, Test Loss: 109.62754058047068\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 886, Train Loss: 106.7451176841289, Test Loss: 109.52790577287865\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 887, Train Loss: 106.6762836801539, Test Loss: 109.42861392407009\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 888, Train Loss: 106.60772988027765, Test Loss: 109.3296637655534\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 889, Train Loss: 106.5394551438671, Test Loss: 109.23105403374055\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 890, Train Loss: 106.47145833493252, Test Loss: 109.1327834699278\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 891, Train Loss: 106.40373832210837, Test Loss: 109.03485082027632\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 892, Train Loss: 106.33629397863456, Test Loss: 108.93725483579303\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 893, Train Loss: 106.26912418233785, Test Loss: 108.83999427231115\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 894, Train Loss: 106.20222781561296, Test Loss: 108.74306789047125\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 895, Train Loss: 106.13560376540411, Test Loss: 108.64647445570228\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 896, Train Loss: 106.06925092318644, Test Loss: 108.55021273820238\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 897, Train Loss: 106.0031681849476, Test Loss: 108.45428151292018\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 898, Train Loss: 105.93735445116938, Test Loss: 108.35867955953586\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 899, Train Loss: 105.87180862680935, Test Loss: 108.26340566244251\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 900, Train Loss: 105.8065296212828, Test Loss: 108.16845861072733\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 901, Train Loss: 105.74151634844435, Test Loss: 108.07383719815317\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 902, Train Loss: 105.67676772657009, Test Loss: 107.97954022314005\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 903, Train Loss: 105.61228267833954, Test Loss: 107.88556648874645\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 904, Train Loss: 105.54806013081762, Test Loss: 107.79191480265129\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 905, Train Loss: 105.48409901543691, Test Loss: 107.69858397713536\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 906, Train Loss: 105.42039826797982, Test Loss: 107.6055728290633\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 907, Train Loss: 105.3569568285609, Test Loss: 107.51288017986529\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 908, Train Loss: 105.29377364160922, Test Loss: 107.4205048555191\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 909, Train Loss: 105.23084765585071, Test Loss: 107.32844568653199\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 910, Train Loss: 105.1681778242909, Test Loss: 107.2367015079229\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 911, Train Loss: 105.10576310419714, Test Loss: 107.14527115920443\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 912, Train Loss: 105.04360245708159, Test Loss: 107.05415348436516\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 913, Train Loss: 104.98169484868379, Test Loss: 106.96334733185199\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 914, Train Loss: 104.9200392489534, Test Loss: 106.87285155455231\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 915, Train Loss: 104.85863463203322, Test Loss: 106.78266500977664\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 916, Train Loss: 104.79747997624197, Test Loss: 106.69278655924099\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 917, Train Loss: 104.73657426405737, Test Loss: 106.60321506904945\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 918, Train Loss: 104.67591648209921, Test Loss: 106.5139494096769\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 919, Train Loss: 104.61550562111245, Test Loss: 106.42498845595154\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 920, Train Loss: 104.55534067595036, Test Loss: 106.33633108703788\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 921, Train Loss: 104.49542064555806, Test Loss: 106.24797618641948\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 922, Train Loss: 104.43574453295557, Test Loss: 106.15992264188192\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 923, Train Loss: 104.37631134522135, Test Loss: 106.0721693454956\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 924, Train Loss: 104.31712009347578, Test Loss: 105.98471519359907\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 925, Train Loss: 104.25816979286469, Test Loss: 105.89755908678195\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 926, Train Loss: 104.19945946254299, Test Loss: 105.81069992986818\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 927, Train Loss: 104.14098812565831, Test Loss: 105.72413663189927\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 928, Train Loss: 104.0827548093348, Test Loss: 105.63786810611766\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 929, Train Loss: 104.02475854465696, Test Loss: 105.55189326995\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 930, Train Loss: 103.96699836665334, Test Loss: 105.46621104499073\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 931, Train Loss: 103.90947331428075, Test Loss: 105.38082035698552\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 932, Train Loss: 103.85218243040808, Test Loss: 105.29572013581496\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 933, Train Loss: 103.7951247618005, Test Loss: 105.21090931547806\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 934, Train Loss: 103.73829935910342, Test Loss: 105.12638683407599\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 935, Train Loss: 103.68170527682688, Test Loss: 105.04215163379614\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 936, Train Loss: 103.62534157332982, Test Loss: 104.9582026608955\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 937, Train Loss: 103.56920731080419, Test Loss: 104.874538865685\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 938, Train Loss: 103.51330155525955, Test Loss: 104.79115920251324\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 939, Train Loss: 103.45762337650753, Test Loss: 104.7080626297506\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 940, Train Loss: 103.40217184814625, Test Loss: 104.6252481097734\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 941, Train Loss: 103.34694604754495, Test Loss: 104.54271460894797\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 942, Train Loss: 103.29194505582868, Test Loss: 104.460461097615\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 943, Train Loss: 103.23716795786291, Test Loss: 104.37848655007372\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 944, Train Loss: 103.18261384223845, Test Loss: 104.29678994456634\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 945, Train Loss: 103.12828180125618, Test Loss: 104.21537026326246\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 946, Train Loss: 103.07417093091192, Test Loss: 104.13422649224351\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 947, Train Loss: 103.02028033088148, Test Loss: 104.05335762148734\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 948, Train Loss: 102.96660910450558, Test Loss: 103.97276264485285\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 949, Train Loss: 102.91315635877515, Test Loss: 103.8924405600646\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 950, Train Loss: 102.85992120431612, Test Loss: 103.81239036869756\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 951, Train Loss: 102.80690275537495, Test Loss: 103.732611076162\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 952, Train Loss: 102.75410012980367, Test Loss: 103.6531016916882\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 953, Train Loss: 102.70151244904537, Test Loss: 103.5738612283114\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 954, Train Loss: 102.64913883811946, Test Loss: 103.49488870285684\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 955, Train Loss: 102.59697842560706, Test Loss: 103.41618313592475\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 956, Train Loss: 102.54503034363678, Test Loss: 103.33774355187553\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 957, Train Loss: 102.49329372787, Test Loss: 103.25956897881464\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 958, Train Loss: 102.44176771748648, Test Loss: 103.18165844857822\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 959, Train Loss: 102.39045145517035, Test Loss: 103.10401099671805\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 960, Train Loss: 102.33934408709548, Test Loss: 103.02662566248694\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 961, Train Loss: 102.28844476291147, Test Loss: 102.94950148882424\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 962, Train Loss: 102.23775263572944, Test Loss: 102.87263752234125\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 963, Train Loss: 102.18726686210803, Test Loss: 102.79603281330657\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 964, Train Loss: 102.13698660203919, Test Loss: 102.71968641563187\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 965, Train Loss: 102.08691101893442, Test Loss: 102.64359738685751\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 966, Train Loss: 102.03703927961067, Test Loss: 102.567764788138\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 967, Train Loss: 101.98737055427657, Test Loss: 102.49218768422801\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 968, Train Loss: 101.93790401651856, Test Loss: 102.41686514346799\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 969, Train Loss: 101.88863884328725, Test Loss: 102.34179623777008\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 970, Train Loss: 101.83957421488361, Test Loss: 102.26698004260408\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 971, Train Loss: 101.79070931494539, Test Loss: 102.19241563698324\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 972, Train Loss: 101.74204333043348, Test Loss: 102.1181021034506\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 973, Train Loss: 101.69357545161853, Test Loss: 102.04403852806466\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 974, Train Loss: 101.64530487206727, Test Loss: 101.97022400038584\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 975, Train Loss: 101.59723078862923, Test Loss: 101.89665761346254\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 976, Train Loss: 101.54935240142336, Test Loss: 101.82333846381738\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 977, Train Loss: 101.50166891382467, Test Loss: 101.75026565143355\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 978, Train Loss: 101.45417953245104, Test Loss: 101.67743827974115\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 979, Train Loss: 101.40688346715001, Test Loss: 101.60485545560356\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 980, Train Loss: 101.35977993098551, Test Loss: 101.53251628930393\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 981, Train Loss: 101.31286814022498, Test Loss: 101.46041989453173\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 982, Train Loss: 101.26614731432616, Test Loss: 101.38856538836927\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 983, Train Loss: 101.2196166759242, Test Loss: 101.31695189127845\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 984, Train Loss: 101.17327545081864, Test Loss: 101.24557852708718\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 985, Train Loss: 101.1271228679606, Test Loss: 101.1744444229764\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 986, Train Loss: 101.08115815943991, Test Loss: 101.10354870946671\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 987, Train Loss: 101.03538056047236, Test Loss: 101.03289052040518\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 988, Train Loss: 100.98978930938699, Test Loss: 100.96246899295242\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 989, Train Loss: 100.94438364761332, Test Loss: 100.89228326756934\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 990, Train Loss: 100.89916281966886, Test Loss: 100.82233248800421\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 991, Train Loss: 100.85412607314647, Test Loss: 100.7526158012798\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 992, Train Loss: 100.80927265870186, Test Loss: 100.68313235768034\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 993, Train Loss: 100.7646018300411, Test Loss: 100.61388131073869\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 994, Train Loss: 100.72011284390817, Test Loss: 100.54486181722362\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 995, Train Loss: 100.67580496007271, Test Loss: 100.47607303712705\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 996, Train Loss: 100.6316774413176, Test Loss: 100.40751413365129\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 997, Train Loss: 100.58772955342671, Test Loss: 100.33918427319654\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 998, Train Loss: 100.54396056517272, Test Loss: 100.27108262534807\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 999, Train Loss: 100.50036974830493, Test Loss: 100.2032083628639\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "forward is well\n",
      "Epoch: 1000, Train Loss: 100.4569563775372, Test Loss: 100.13556066166223\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "train_loss_list, test_loss_list = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = train_epoch(model, criterion, optimizer, X_train, y_train, batch_size)\n",
    "    test_losses = test_epoch(model, X_test, y_test, batch_size)\n",
    "    train_loss = sum(train_losses) / len(train_losses)\n",
    "    test_loss = sum(test_losses) / len(test_losses)\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    print(f'Epoch: {epoch + 1}, Train Loss: {train_loss}, Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_list, color='b', label='Train Loss')\n",
    "plt.plot(test_loss_list, color='r', label='Test Loss')\n",
    "plt.ylim((0, 1000))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss Value')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
